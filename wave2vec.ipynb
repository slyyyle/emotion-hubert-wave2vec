{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Audio, DatasetDict, load_metric\n",
    "from transformers import Wav2Vec2FeatureExtractor, HubertModel, HubertConfig, HubertForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, Wav2Vec2Processor\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure CUDA is available!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a custom metadata given the following naming convention for our data files:\n",
    "\n",
    "Example: 03-01-01-01-01-01-01.wav\n",
    "\n",
    "Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "\n",
    "Vocal channel (01 = speech, 02 = song).\n",
    "\n",
    "Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "\n",
    "Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "\n",
    "Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "\n",
    "Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "\n",
    "Actor (01 to 24. Odd numbered actors are male, even numbered actors are female)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_folder = r'C:\\Git Repos\\s2t-wave2vec\\data'\n",
    "\n",
    "data = []\n",
    "\n",
    "for filename in os.listdir(audio_folder):\n",
    "    if filename.endswith('.wav'):\n",
    "        file_name = filename\n",
    "        modality = str(filename[0:2])\n",
    "        vocal_channel = str(filename[3:5])\n",
    "        emotion = str(filename[6:8])\n",
    "        intensity = str(filename[9:11])\n",
    "        statement = str(filename[12:14])\n",
    "        repetition = str(filename[15:17])\n",
    "        actor = str(filename[18:20])\n",
    "\n",
    "    data.append([file_name, modality, vocal_channel, emotion, intensity, statement, repetition, actor])\n",
    "\n",
    "column_names = ['file_name','Modality',\"Vocal Channel\",\"Emotion\",\"Intensity\",\"Statement\",\"Repetition\",\"Actor\"]\n",
    "metadata_df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "emotion_mapping = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "intensity_mapping = {\n",
    "    '01':'normal',\n",
    "    '02':'strong'\n",
    "}\n",
    "\n",
    "statement_mapping = {\n",
    "    '01':'Kids are talking by the door',\n",
    "    '02':'Dogs are sitting by the door'\n",
    "}\n",
    "\n",
    "repetition_mapping = {\n",
    "    '01':'First',\n",
    "    '02':'Second'\n",
    "}\n",
    "\n",
    "modality_mapping = {\n",
    "    '03':'audio'\n",
    "}\n",
    "\n",
    "vocalchannel_mapping = {\n",
    "    '01':\"speech\"\n",
    "}\n",
    "\n",
    "metadata_df.replace({\n",
    "    'Modality': modality_mapping,\n",
    "    'Vocal Channel':vocalchannel_mapping,\n",
    "    'Emotion': emotion_mapping,\n",
    "    'Intensity': intensity_mapping,\n",
    "    'Statement': statement_mapping,\n",
    "    'Repetition': repetition_mapping,\n",
    "    # Add other column mappings if needed\n",
    "}, inplace=True)\n",
    "\n",
    "metadata_df['Gender'] = metadata_df['Actor'].apply(lambda x: 'Male' if int(x) % 2 == 1 else 'Female')\n",
    "\n",
    "metadata_df.to_csv('data/metadata.csv', index=False)\n",
    "\n",
    "label_list = [i for i in metadata_df['Emotion'].unique()]\n",
    "label_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can always repurpose this notebook for different tasks as we have multiple columns with possible labels!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Custom Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FIELD = \"input_values\"\n",
    "LABEL_FIELD = \"labels\"\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(\n",
    "        self, examples: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        input_features = [\n",
    "            {INPUT_FIELD: example[INPUT_FIELD]} for example in examples\n",
    "        ]  # example is basically row0, row1, etc...\n",
    "        labels = [example[LABEL_FIELD] for example in examples]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        batch[LABEL_FIELD] = torch.tensor(labels)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in Wave2Vec Assets and Custom Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/hubert-large-ls960-ft were not used when initializing HubertModel: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/hubert-large-ls960-ft were not used when initializing HubertForSequenceClassification: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing HubertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['projector.weight', 'projector.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/hubert-large-ls960-ft\"\n",
    "NUM_LABELS = 8\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor().from_pretrained(model_name)\n",
    "sampling_rate = feature_extractor.sampling_rate\n",
    "hubert_base = HubertModel.from_pretrained(model_name)\n",
    "hubert_config = HubertConfig.from_pretrained(model_name, num_labels=NUM_LABELS)\n",
    "hubert_model = HubertForSequenceClassification.from_pretrained(model_name, config=hubert_config, ignore_mismatched_sizes=True)\n",
    "data_collator = DataCollatorCTCWithPadding(processor=feature_extractor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0,\n",
      "  \"return_attention_mask\": true,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HubertModel(\n",
      "  (feature_extractor): HubertFeatureEncoder(\n",
      "    (conv_layers): ModuleList(\n",
      "      (0): HubertLayerNormConvLayer(\n",
      "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "      (1-4): 4 x HubertLayerNormConvLayer(\n",
      "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "      (5-6): 2 x HubertLayerNormConvLayer(\n",
      "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
      "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (feature_projection): HubertFeatureProjection(\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): HubertEncoderStableLayerNorm(\n",
      "    (pos_conv_embed): HubertPositionalConvEmbedding(\n",
      "      (conv): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
      "      (padding): HubertSamePadLayer()\n",
      "      (activation): GELUActivation()\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x HubertEncoderLayerStableLayerNorm(\n",
      "        (attention): HubertAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): HubertFeedForward(\n",
      "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "          (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(hubert_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HubertForSequenceClassification(\n",
      "  (hubert): HubertModel(\n",
      "    (feature_extractor): HubertFeatureEncoder(\n",
      "      (conv_layers): ModuleList(\n",
      "        (0): HubertLayerNormConvLayer(\n",
      "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (1-4): 4 x HubertLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (5-6): 2 x HubertLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (feature_projection): HubertFeatureProjection(\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): HubertEncoderStableLayerNorm(\n",
      "      (pos_conv_embed): HubertPositionalConvEmbedding(\n",
      "        (conv): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
      "        (padding): HubertSamePadLayer()\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x HubertEncoderLayerStableLayerNorm(\n",
      "          (attention): HubertAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): HubertFeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projector): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (classifier): Linear(in_features=256, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(hubert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataCollatorCTCWithPadding(processor=Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0,\n",
      "  \"return_attention_mask\": true,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      ", padding=True, max_length=None, max_length_labels=None, pad_to_multiple_of=None, pad_to_multiple_of_labels=None)\n"
     ]
    }
   ],
   "source": [
    "print(data_collator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to create our DatasetDict object.  This finds \"metadata.csv\" and maps its data to each file in the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa3bfc153b542898a3f0d639e3f7d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1441 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset audiofolder/default to C:/Users/khamm/.cache/huggingface/datasets/audiofolder/default-3e24c60e40fc04af/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce89d78d9514e0983d4b921363c43ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1441 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595fbe4eb5bf4b968bb962dcfc1fb6e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d623080d4744b7a436953ed9ac81c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abba7d94e140478c8f70051ccf4a7522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset audiofolder downloaded and prepared to C:/Users/khamm/.cache/huggingface/datasets/audiofolder/default-3e24c60e40fc04af/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1ee7fe6f8e4c0b817b9c374dc49444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'audio_arrays'],\n",
       "        num_rows: 1440\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wanted to make this general to try a few different fine tuning tasks in the future\n",
    "#this allows me to change \"Emotion\" out for a different column, and have the script run no matter what it's called without changing the column name\n",
    "#annoying that the Datasets package doesn't have a 'select_columns' functionality\n",
    "select_columns = ['audio','Emotion']\n",
    "output_column = select_columns[1]\n",
    "\n",
    "#load dataset and remove undesired columns (see above)\n",
    "dataset = load_dataset(\"audiofolder\", data_dir=\"data\")\n",
    "cols_to_remove = [i for i in dataset['train'].column_names if i not in select_columns]\n",
    "dataset = dataset.remove_columns(cols_to_remove)\n",
    "\n",
    "#rename columns to what the trainer object expects\n",
    "dataset = dataset.rename_column(select_columns[1],\"label\")\n",
    "\n",
    "#casts audio column to sampling rate expected by feature_extractor\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n",
    "\n",
    "#extract arrays and remove nested audio column\n",
    "audio_arrays = [np.array(item[\"array\"]) for item in dataset['train'][\"audio\"]]\n",
    "dataset['train'] = dataset['train'].add_column(\"audio_arrays\",audio_arrays)\n",
    "dataset = dataset.remove_columns(\"audio\")\n",
    "\n",
    "#view dataset\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepares Dataset by Utilizing Feature Extractor and Adding Extracted Features and Labels to DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bdf78ef295344f0905cb72a793d0ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_values', 'labels'],\n",
       "        num_rows: 1440\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_dataset(batch, feature_extractor):\n",
    "    audio_arr = batch[\"audio_arrays\"]\n",
    "    input = feature_extractor(\n",
    "        audio_arr, sampling_rate=sampling_rate, padding=True, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    batch[INPUT_FIELD] = input.input_values[0]\n",
    "    batch[LABEL_FIELD] = batch[\"label\"]  # colname MUST be labels as Trainer will look for it by default\n",
    "\n",
    "    return batch\n",
    "\n",
    "# APPLY THE DATA PREP USING FEATURE EXTRACTOR TO ALL EXAMPLES\n",
    "dataset = dataset.map(\n",
    "    prepare_dataset,\n",
    "    fn_kwargs={\"feature_extractor\": feature_extractor},\n",
    "    num_proc=1,\n",
    ")\n",
    "\n",
    "dataset = dataset.remove_columns([\"audio_arrays\",\"label\"])\n",
    "\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding Class Labels and Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2fac85e0b44c9998cc6be22a86925a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/1440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_values', 'labels'],\n",
       "        num_rows: 1152\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_values', 'labels'],\n",
       "        num_rows: 144\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_values', 'labels'],\n",
       "        num_rows: 144\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encode labels column\n",
    "dataset['train'] = dataset['train'].class_encode_column('labels')\n",
    "\n",
    "#train test split\n",
    "train_testvalid = dataset['train'].train_test_split(shuffle=True, test_size=0.2)\n",
    "test_valid = train_testvalid['test'].train_test_split(shuffle=True,test_size=0.5)\n",
    "dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'val': test_valid['train']\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer Config and Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = {\n",
    "  \"OUTPUT_DIR\": \"results\",\n",
    "  \"TRAIN_EPOCHS\": 3,\n",
    "  \"TRAIN_BATCH_SIZE\": 4,\n",
    "  \"EVAL_BATCH_SIZE\": 4,\n",
    "  \"GRADIENT_ACCUMULATION_STEPS\": 4,\n",
    "  \"WARMUP_STEPS\": 500,\n",
    "  \"DECAY\": 0.01,\n",
    "  \"LOGGING_STEPS\": 10,\n",
    "  \"MODEL_DIR\": \"models/slyle-test-hubert-model-ravdess\",\n",
    "  \"SAVE_STEPS\": 100\n",
    "}\n",
    "\n",
    "# Fine-Tuning with Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=trainer_config[\"OUTPUT_DIR\"],  # output directory\n",
    "    gradient_accumulation_steps=trainer_config[\n",
    "        \"GRADIENT_ACCUMULATION_STEPS\"\n",
    "    ],  # accumulate the gradients before running optimization step\n",
    "    num_train_epochs=trainer_config[\n",
    "        \"TRAIN_EPOCHS\"\n",
    "    ],  # total number of training epochs\n",
    "    per_device_train_batch_size=trainer_config[\n",
    "        \"TRAIN_BATCH_SIZE\"\n",
    "    ],  # batch size per device during training\n",
    "    per_device_eval_batch_size=trainer_config[\n",
    "        \"EVAL_BATCH_SIZE\"\n",
    "    ],  # batch size for evaluation\n",
    "    warmup_steps=trainer_config[\n",
    "        \"WARMUP_STEPS\"\n",
    "    ],  # number of warmup steps for learning rate scheduler\n",
    "    save_steps=trainer_config[\"SAVE_STEPS\"], # save checkpoint every 100 steps\n",
    "    weight_decay=trainer_config[\"DECAY\"],  # strength of weight decay\n",
    "    logging_steps=trainer_config[\"LOGGING_STEPS\"],\n",
    "    evaluation_strategy=\"epoch\",  # report metric at end of each epoch\n",
    "    report_to=\"wandb\",  # enable logging to W&B\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    # DEFINE EVALUATION METRIC\n",
    "    compute_accuracy_metric = load_metric(\"accuracy\")\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return compute_accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Trainer Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=hubert_model,  # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,  # training arguments, defined above\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset[\"train\"],  # training dataset\n",
    "    eval_dataset=dataset[\"val\"],  # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting to WANDB Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkhammitt1\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Git Repos\\s2t-wave2vec\\wandb\\run-20230530_173753-zqfmocd2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/khammitt1/finetuning_wave2vec_emotion/runs/zqfmocd2' target=\"_blank\">sunny-moon-9</a></strong> to <a href='https://wandb.ai/khammitt1/finetuning_wave2vec_emotion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/khammitt1/finetuning_wave2vec_emotion' target=\"_blank\">https://wandb.ai/khammitt1/finetuning_wave2vec_emotion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/khammitt1/finetuning_wave2vec_emotion/runs/zqfmocd2' target=\"_blank\">https://wandb.ai/khammitt1/finetuning_wave2vec_emotion/runs/zqfmocd2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/khammitt1/finetuning_wave2vec_emotion/runs/zqfmocd2?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1ec2ae47dd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USER = 'khammitt1'\n",
    "WANDB_PROJECT = \"finetuning_wave2vec_emotion\"\n",
    "WANDB_NOTEBOOK_NAME = 'wave2vec.ipynb'\n",
    "wandb.init(entity=USER, project=WANDB_PROJECT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING TIME!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Git Repos\\s2t-wave2vec\\venv\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f38df807df44a8eadf536b2b1cd7f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/216 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0799, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.14}\n",
      "{'loss': 2.0782, 'learning_rate': 1.5e-06, 'epoch': 0.28}\n",
      "{'loss': 2.0828, 'learning_rate': 2.5e-06, 'epoch': 0.42}\n",
      "{'loss': 2.0796, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.56}\n",
      "{'loss': 2.0844, 'learning_rate': 4.4e-06, 'epoch': 0.69}\n",
      "{'loss': 2.0828, 'learning_rate': 5.4e-06, 'epoch': 0.83}\n",
      "{'loss': 2.0765, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae95d498a1c440ca9212cbfa411a12a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khamm\\AppData\\Local\\Temp\\ipykernel_30712\\3230840777.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  compute_accuracy_metric = load_metric(\"accuracy\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0726046562194824, 'eval_accuracy': 0.14583333333333334, 'eval_runtime': 25.6546, 'eval_samples_per_second': 5.613, 'eval_steps_per_second': 1.403, 'epoch': 1.0}\n",
      "{'loss': 2.0782, 'learning_rate': 7.4e-06, 'epoch': 1.11}\n",
      "{'loss': 2.0766, 'learning_rate': 8.400000000000001e-06, 'epoch': 1.25}\n",
      "{'loss': 2.0699, 'learning_rate': 9.4e-06, 'epoch': 1.39}\n",
      "{'loss': 2.0636, 'learning_rate': 1.04e-05, 'epoch': 1.53}\n",
      "{'loss': 2.0652, 'learning_rate': 1.1400000000000001e-05, 'epoch': 1.67}\n",
      "{'loss': 2.064, 'learning_rate': 1.24e-05, 'epoch': 1.81}\n",
      "{'loss': 2.0592, 'learning_rate': 1.3300000000000001e-05, 'epoch': 1.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2ec5a664e044e8afbb360e7cc72c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0374484062194824, 'eval_accuracy': 0.24305555555555555, 'eval_runtime': 50.0995, 'eval_samples_per_second': 2.874, 'eval_steps_per_second': 0.719, 'epoch': 2.0}\n",
      "{'loss': 2.0443, 'learning_rate': 1.43e-05, 'epoch': 2.08}\n",
      "{'loss': 2.0393, 'learning_rate': 1.53e-05, 'epoch': 2.22}\n",
      "{'loss': 2.0401, 'learning_rate': 1.63e-05, 'epoch': 2.36}\n",
      "{'loss': 2.0121, 'learning_rate': 1.73e-05, 'epoch': 2.5}\n",
      "{'loss': 1.9817, 'learning_rate': 1.83e-05, 'epoch': 2.64}\n",
      "{'loss': 1.9503, 'learning_rate': 1.93e-05, 'epoch': 2.78}\n",
      "{'loss': 1.9151, 'learning_rate': 2.0200000000000003e-05, 'epoch': 2.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972238454568454c95a24778da22b4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8321126699447632, 'eval_accuracy': 0.3611111111111111, 'eval_runtime': 30.4222, 'eval_samples_per_second': 4.733, 'eval_steps_per_second': 1.183, 'epoch': 3.0}\n",
      "{'train_runtime': 2827.2389, 'train_samples_per_second': 1.222, 'train_steps_per_second': 0.076, 'train_loss': 2.043682239673756, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=216, training_loss=2.043682239673756, metrics={'train_runtime': 2827.2389, 'train_samples_per_second': 1.222, 'train_steps_per_second': 0.076, 'train_loss': 2.043682239673756, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "# TO RESUME TRAINING FROM CHECKPOINT\n",
    "# trainer.train(\"results/checkpoint-2000\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where the example ends.  At this point, we see the model is steadily learning, and after 3 epochs it has already achieved 36% accuracy in 47 minutes of training time from a starting accuracy of a random guess (0.125).  You might be thinking, why did you do all this work for such measly accuracy when pre-trained models on this exact task exist on huggingface?  \n",
    "\n",
    "    -It's just for practice, and now I can repurpose this work for other tasks.  \n",
    "    -There will come a day when I will have custom data and a problem no one else has tackled, which will require fine tuning!  \n",
    "    -I will use those pre-trained models when learning how to deploy and use RESTful API endpoints.  Hopefully I can host those locally, because I don't have $$  \n",
    "\n",
    "However, we cannot extrapolate this performance to MANY further epochs (maybe a few), and have a few issues that we may run into with more epochs:\n",
    "\n",
    "    -The model will eventually overfit as we only have about 1200 training samples and <100 samples per class  \n",
    "    -My girlfriend will kill me if I continue to run our power bill through the roof  \n",
    "    -I have maxed out my VRAM at a batch size of 4, using mixed precision (fp16), which may limit performance in further epochs (this depends on learning rate schedule, and may not occur)\n",
    "\n",
    "If I were to continue the example, I would:\n",
    "\n",
    "    -Reduce the # of warmup steps and increase learning rate, as it appeared to benefit as learning rate increased.  \n",
    "    -Find a larger labeled dataset, although this may limit the granularity of our emotion labels (I could use the full RAVDESS dataset, as it is much larger than this subset)  \n",
    "    -Increase learning rate  \n",
    "    -Get a new graphics card and increase batch size (NVIDIA 4090 has 24GB which looks very appetizing right now, but is 1700 dollars and I have to pay rent)  \n",
    "    -Try out other models in addition to HUBERT\n",
    "\n",
    "Things to note:  \n",
    "\n",
    "    -load_metrics will be deprecated soon, the code will have to be altered in the future in order to report metrics using evaluate.load  \n",
    "    -I will need to figure out how to use torch.optim.AdamW as the current implementation of AdamW is deprecated  \n",
    "    -Transformers v4.28.0 had to be used for this example, as future versions have been reported unstable (Partial State error occurs with v4.29.2 in TrainingArguments class).  It is unclear whether or not this error is in v4.29.0, but I used 4.28.0 at the recommendations of a forum post I saw.  \n",
    "    -There has to be a more elegant way to use huggingface's dataset package, as you saw previously I had to really wrangle with it in order to get it in the correct state for training.  It felt awkward the whole way through\n",
    "\n",
    "Things still to learn:\n",
    "\n",
    "    -I need to get a grasp on how to separate this notebook into .py scripts, especially if I want to offload this to an Azure job\n",
    "    -I need to practice Azure jobs, using simpler models.  I would really like to use this example, but I can't get my hands on compute that would outperform my 3080 (thanks a lot, Sam Altman)\n",
    "    -I need to learn to deploy API endpoints, but these types of models would run me 400+ bucks per month on huggingface and probably about the same on Azure\n",
    "    -We defined a custom DataCollator in this example, but I need to do more research to figure out if a prepackaged DataCollator from HuggingFace does the exact same thing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
